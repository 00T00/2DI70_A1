{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2DI70 Statistical Learning Theory \n",
    "## Assignment 1\n",
    "\n",
    "### Group x\n",
    "Name1 ID1\n",
    "\n",
    "Name2 ID2\n",
    "\n",
    "Name3 ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MNIST data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = pd.read_csv(\"MNIST_train_small.csv\").values\n",
    "test_small  = pd.read_csv(\"MNIST_test_small.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing the data is probably not necessary for this specific dataset\n",
    "X_train_s = train_small[:, 1:]/255.0\n",
    "y_train_s = train_small[:,0]\n",
    "\n",
    "X_test_s = test_small[:, 1:]/255.0\n",
    "y_test_s = test_small[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with training dataset\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train_s)\n",
    "print(pca.n_components_)\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Small Training Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above plot and pca fitting tells us that selecting 145 components we can preserve something around 95% of the total variance\n",
    "#of the data. It makes sense, weâ€™ll not use 100% of our variance, because it denotes all components,\n",
    "#and we want only the principal ones.\n",
    "# With this information in our hands, we can implement the PCA for 145 best components by using this snippet of code:\n",
    "\n",
    "PCA_X_train_s = pca.transform(X_train_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the nth root of given value\n",
    "def n_root(val, p):   \n",
    "    return val**(1 / float(p))\n",
    "  \n",
    "## Compute Minkowski distance for two given vectors\n",
    "## Use p = 2 for Euclidian distance\n",
    "def minkowski_dist(x1, x2, p):    \n",
    "    p_sum = sum(pow(abs(a-b), p) for a, b in zip(x1, x2))\n",
    "    return n_root(p_sum, p) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute p-minkowski distance from point to every datapoint in collection\n",
    "## The list of labels should share length and indices with collection\n",
    "def compute_dists(point, collection, labels, p):\n",
    "    vals = []\n",
    "    for i in range(len(collection)):\n",
    "        d = minkowski_dist(point, collection[i], p)\n",
    "        vals.append([i, d, labels[i]])\n",
    "    \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do KNN magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1g. Using new dataset of PCA_X_train_s to reduce 784 dimensions on original X_train_s to 200 dimensions\n",
    "#Apply the new dataset using pure k-nn, k = 20\n",
    "#Predefine number of top k-nn, power p for Euclidean or Minkowski distance, length of training folds\n",
    "k = 4\n",
    "p = 2 \n",
    "r = len(X_train_s)\n",
    "error = 0\n",
    "\n",
    "#for each point in train set of n values, compute the distance with all n-1 data points in training set (excluding the point selected)\n",
    "# then sort in distance for top 20 nearest neighbors\n",
    "for i in range(r):\n",
    "    dists = compute_dists(PCA_X_train_s[i], PCA_X_train_s, y_train_s, p)\n",
    "    dists_sorted = sorted(dists, key = lambda x: float(x[1]))\n",
    "    list = dists_sorted[0:k] #get list top 20 nearest neighbors with its [index,distance,label] \n",
    "     \n",
    "#count frequency of labels of the top 20 nearest neighbors:\n",
    "    pred_class = {}\n",
    "    for j in range(k):\n",
    "        labels = list[j][-1]\n",
    "        if labels in pred_class:\n",
    "            pred_class[labels] += 1\n",
    "        else:\n",
    "            pred_class[labels] = 1\n",
    "        #sort the most frequency labels at the beginning, return the prediction as the most frequency label at first position\n",
    "    sortedVotes = sorted(pred_class.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    if sortedVotes[0][0] != y_train_s[i]:\n",
    "        error = error + 1\n",
    "    print(sortedVotes[0][0],y_train_s[i],error)\n",
    "print(\"total percentage of error\" + str(error/r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
